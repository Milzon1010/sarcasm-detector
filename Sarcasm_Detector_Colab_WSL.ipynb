{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Sarcasm Detector — Colab/WSL Ready (BiLSTM+MaxPool)\n**One-click** notebook to load data (TFDS or CSV), train, evaluate (ROC‑AUC, best‑F1 threshold), and save artifacts (vocab, meta, model).  \n> Works in **Colab** (GPU/CPU) and **local WSL** (CPU). If Colab GPU quota is unavailable, it will still run on CPU."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Setup & environment check"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os, sys, random, numpy as np, tensorflow as tf\nSEED=42\nrandom.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n\nIN_COLAB = \"google.colab\" in sys.modules\nprint(\"Python :\", sys.version)\nprint(\"TF     :\", tf.__version__, \"(Colab:\", IN_COLAB, \")\")\n\n# If running on Colab: upgrade/install deps to latest stable\nif IN_COLAB:\n    # Light install (keeps Colab runtime defaults; uncomment if you need exact versions)\n    # !pip -q install \"tensorflow==2.19.*\" \"tensorflow-text==2.19.*\" scikit-learn pandas matplotlib pyyaml emoji\n    try:\n        import tensorflow_text as text\n    except Exception as e:\n        print(\"Installing tensorflow-text for Colab...\")\n        !pip -q install \"tensorflow-text==2.19.*\"\n        import tensorflow_text as text\nelse:\n    # Local/WSL guidance\n    print(\"Local/WSL detected. If TF 2.19 has dependency issues, use 2.18:\")\n    print(\"  python3 -m pip install 'tensorflow==2.18.0' 'tensorflow-text==2.18.0' scikit-learn pandas matplotlib pyyaml emoji\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Imports"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import re, json, pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\ntry:\n    import tensorflow_text as text  # noqa: F401\n    print(\"TF-Text OK\")\nexcept Exception as e:\n    print(\"TF-Text not available:\", e)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Tokenizer with custom standardization (keep emoji, `!` and `?`)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def custom_standardize(s):\n    # lowercase & keep unicode (emoji), ! and ?\n    s = tf.strings.lower(s)\n    s = tf.strings.regex_replace(s, r\"[^a-z0-9!\\?\\s\\u0080-\\uFFFF]\", \" \")\n    return s\n\ndef build_vectorizer(max_tokens=30000, seq_len=64, vocab=None):\n    vec = layers.TextVectorization(\n        max_tokens=max_tokens,\n        output_mode=\"int\",\n        output_sequence_length=seq_len,\n        standardize=custom_standardize,\n    )\n    if vocab is not None:\n        vec.set_vocabulary(vocab)\n    return vec"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Load data — TFDS *sarcasm* if available, else CSV on Drive"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "USE_TFDS = True\ndf = None\nif USE_TFDS:\n    try:\n        import tensorflow_datasets as tfds\n        ds_tr, ds_te = tfds.load(\"sarcasm\", split=[\"train\",\"test\"], as_supervised=True)\n        tr = [(x.numpy().decode(\"utf-8\"), int(y.numpy())) for x,y in tfds.as_numpy(ds_tr)]\n        te = [(x.numpy().decode(\"utf-8\"), int(y.numpy())) for x,y in tfds.as_numpy(ds_te)]\n        import pandas as pd\n        df = pd.DataFrame(tr+te, columns=[\"text\",\"label\"])\n        print(\"Loaded TFDS 'sarcasm' →\", df.shape)\n    except Exception as e:\n        print(\"TFDS 'sarcasm' not available:\", e)\n        USE_TFDS = False\n\nif not USE_TFDS:\n    # If you are on Colab, mount Drive and point to your CSV (must have 'text','label' columns)\n    if \"google.colab\" in sys.modules:\n        from google.colab import drive\n        drive.mount('/content/drive', force_remount=True)\n        PATH = \"/content/drive/MyDrive/sarcasm-detector/data/raw/sarcasm.csv\"  # change if needed\n    else:\n        PATH = \"./data/raw/sarcasm.csv\"  # change if running locally\n\n    df = pd.read_csv(PATH)\n    assert set(df.columns) >= {\"text\",\"label\"}, \"CSV must contain 'text' and 'label' columns\"\n    print(\"Loaded CSV →\", df.shape, \"from\", PATH)\n\ndf.head()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Split train/val/test + class weights"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "X_train, X_tmp, y_train, y_tmp = train_test_split(\n    df[\"text\"].astype(str).values, df[\"label\"].astype(int).values,\n    test_size=0.2, stratify=df[\"label\"], random_state=SEED\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_tmp, y_tmp, test_size=0.5, stratify=y_tmp, random_state=SEED\n)\n\nclasses = np.unique(y_train)\ncw = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\nclass_weight = {int(c): float(w) for c, w in zip(classes, cw)}\nprint(\"Class weight:\", class_weight)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Vectorize + simple handcrafted features"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "max_tokens, seq_len = 30000, 64\nvec = build_vectorizer(max_tokens, seq_len)\nvec.adapt(tf.constant(X_train))\n\nEMOJI_RE = re.compile(r\":\\)|:\\(|😂|😒|😑|😏|🙄\")\ndef basic_features(text):\n    caps_ratio = sum(c.isupper() for c in text)/max(1,len(text))\n    punct_burst = int((\"!!!\" in text) or (\"???\" in text))\n    emoji_flag = int(bool(EMOJI_RE.search(text)))\n    return np.array([punct_burst, caps_ratio, emoji_flag], dtype=\"float32\")\n\ndef batch_features(texts):\n    return np.stack([basic_features(t) for t in texts], axis=0)\n\ntok_tr = vec(tf.constant(X_train)).numpy()\ntok_va = vec(tf.constant(X_val)).numpy()\ntok_te = vec(tf.constant(X_test)).numpy()\nf_tr, f_va, f_te = batch_features(X_train), batch_features(X_val), batch_features(X_test)\n\ntok_tr.shape, f_tr.shape"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Model — BiLSTM + GlobalMaxPool (robust & simple)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "text_in = keras.Input(shape=(seq_len,), dtype=tf.int32, name=\"tok\")\nx = layers.Embedding(max_tokens, 128, mask_zero=True)(text_in)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\nx = layers.GlobalMaxPool1D()(x)\n\nfeat_in = keras.Input(shape=(3,), dtype=\"float32\", name=\"feat\")\nh = layers.Concatenate()([x, feat_in])\nh = layers.Dense(64, activation=\"relu\")(h)\nh = layers.Dropout(0.3)(h)\nout = layers.Dense(1, activation=\"sigmoid\")(h)\n\nmodel = keras.Model([text_in, feat_in], out)\nmodel.compile(optimizer=keras.optimizers.Adam(1e-3),\n              loss=\"binary_crossentropy\",\n              metrics=[keras.metrics.BinaryAccuracy(name=\"acc\"), keras.metrics.AUC(name=\"auc\")])\nmodel.summary()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Train with callbacks"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "callbacks = [\n    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n]\n\nhist = model.fit(\n    {\"tok\": tok_tr, \"feat\": f_tr}, y_train,\n    validation_data=({\"tok\": tok_va, \"feat\": f_va}, y_val),\n    epochs=15, batch_size=128,\n    class_weight=class_weight, callbacks=callbacks, verbose=1\n)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9) Evaluate (ROC‑AUC, best‑F1 threshold, report)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def eval_split(model, tok, feats, y_true, name=\"VAL\"):\n    prob = model.predict({\"tok\": tok, \"feat\": feats}, verbose=0).ravel()\n    prec, rec, thr = precision_recall_curve(y_true, prob)\n    f1s = 2*prec*rec/(prec+rec+1e-9)\n    thr_opt = float(thr[np.argmax(f1s[:-1])]) if len(thr)>0 else 0.5\n    y_pred = (prob >= thr_opt).astype(int)\n    print(f\"\\n[{name}] ROC-AUC: {roc_auc_score(y_true, prob):.4f} | Best-F1 thr: {thr_opt:.3f}\")\n    print(classification_report(y_true, y_pred, digits=4))\n    print(\"Confusion:\\n\", confusion_matrix(y_true, y_pred))\n    return thr_opt\n\nthr_val = eval_split(model, tok_va, f_va, y_val, \"VAL\")\n_ = eval_split(model, tok_te, f_te, y_test, \"TEST\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10) Save artifacts (vocab, meta, model)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "BASE = \"/content/drive/MyDrive/sarcasm-detector\" if IN_COLAB else \"./sarcasm-detector\"\nPROC = f\"{BASE}/data/processed\"\nos.makedirs(f\"{PROC}/models\", exist_ok=True)\n\n# vocab\nvocab = vec.get_vocabulary()\nwith open(f\"{PROC}/vocab.txt\",\"w\") as f: f.write(\"\\n\".join(vocab))\n\n# meta\nmeta = {\"max_tokens\": int(max_tokens), \"seq_len\": int(seq_len),\n        \"features\":[\"punct_burst\",\"caps_ratio\",\"emoji_flag\"],\n        \"thr_val\": float(thr_val)}\nwith open(f\"{PROC}/meta.json\",\"w\") as f: json.dump(meta, f, indent=2)\n\n# model\nmodel.save(f\"{PROC}/models/bilstm_maxpool.keras\")\nprint(\"Saved →\", f\"{PROC}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11) Inference helper"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def rebuild_vec(vocab, seq_len):\n    v = layers.TextVectorization(max_tokens=len(vocab), output_mode=\"int\",\n                                 output_sequence_length=seq_len, standardize=custom_standardize)\n    v.set_vocabulary(vocab)\n    return v\n\ndef features_batch(texts):\n    EMOJI_RE = re.compile(r\":\\)|:\\(|😂|😒|😑|😏|🙄\")\n    def basic_features(text):\n        caps_ratio = sum(c.isupper() for c in text)/max(1,len(text))\n        punct_burst = int((\"!!!\" in text) or (\"???\" in text))\n        emoji_flag = int(bool(EMOJI_RE.search(text)))\n        return np.array([punct_burst, caps_ratio, emoji_flag], dtype=\"float32\")\n    return np.stack([basic_features(t) for t in texts], axis=0)\n\ndef load_artifacts(base_dir):\n    PROC = f\"{base_dir}/data/processed\"\n    model = keras.models.load_model(f\"{PROC}/models/bilstm_maxpool.keras\")\n    vocab = open(f\"{PROC}/vocab.txt\").read().splitlines()\n    meta = json.load(open(f\"{PROC}/meta.json\"))\n    vec2 = rebuild_vec(vocab, meta[\"seq_len\"])\n    return model, vec2, meta\n\nBASE = \"/content/drive/MyDrive/sarcasm-detector\" if IN_COLAB else \"./sarcasm-detector\"\nloaded_model, vec2, meta = load_artifacts(BASE)\n\ndef predict_texts(texts, thr=None):\n    if thr is None: thr = meta.get(\"thr_val\", 0.5)\n    tok = vec2(tf.constant(texts)).numpy()\n    feats = features_batch(texts)\n    prob = loaded_model.predict({\"tok\": tok, \"feat\": feats}, verbose=0).ravel()\n    pred = (prob >= thr).astype(int)\n    return prob, pred\n\nsamples = [\"Great, another mandatory meeting at 7am. Perfect.\", \"Terima kasih atas bantuannya.\"]\nprobs, preds = predict_texts(samples)\nfor s, p, pr in zip(samples, preds, probs):\n    print(f\"{s[:70]} → pred={p} prob={pr:.3f}\")"
    }
  ]
}